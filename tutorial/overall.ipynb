{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LotsoTeddy/ArkIntelligence/blob/tutorial/tutorial/overall.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG33oR-V0u25"
      },
      "source": [
        "<hr/>\n",
        "<img src=\"https://portal.volccdn.com/obj/volcfe/logo/appbar_logo_dark.2.svg?sanitize=true\" align=center>\n",
        "<hr/>\n",
        "\n",
        "# 🤗 Introduction\n",
        "\n",
        "Volengine Ark provides you with a development platform for large model services, offering feature-rich, secure and price-competitive model calling services, as well as end-to-end functions such as model data, fine-tuning, reasoning, evaluation, and so on, to comprehensively guarantee your AI application development landing.\n",
        "\n",
        "This is a freshman-friendly tutorial for Ark SDK, which helps you to build your own intelligent applications through agent, knowledge base, and so on.\n",
        "\n",
        "**Github:** Click [here](https://github.com/LotsoTeddy/ArkIntelligence/) to explore the Work-In-Progress Github repository"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ4wkrB4Hrr4"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iveInZ44HruU"
      },
      "source": [
        "### Why Ark?\n",
        "\n",
        "Ark is a platform that supports multiple kinds of models running. Ark has the following advantages:\n",
        "\n",
        "- **Security and Mutual Trust**: Large model security and trust program strictly protects the model and information security of model providers and model users, click to view the white paper on security and mutual trust.\n",
        "- **Selected Models**: Supporting multi-industry models for various business scenarios, providing rich platform applications and tools to help you build your own innovative scenarios.\n",
        "- **Strong Arithmetic Power**: Based on the volcano's Wanka resource pool, we provide sufficient high-performance GPU resources to provide you with end-to-end modeling services including model fine-tuning, evaluation, and inference.\n",
        "- **Enterprise-level services**: provide professional service system support, professional product operation and sales delivery services to meet the needs of enterprise application construction and delivery."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDoc86MlHr1M"
      },
      "source": [
        "### Productions\n",
        "\n",
        "Productions in ARK, including models, agents and something else. The specific productions are shown in the following image.\n",
        "\n",
        "![productions](https://ark-tutorial.tos-cn-beijing.volces.com/assets/images/productions.png)\n",
        "\n",
        "Rodemap and primary changelog of Ark.\n",
        "\n",
        "- 2025-03-31: foo\n",
        "- 2025-03-30: bar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3EFGnfhHr3s"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnUjrM_NHr51"
      },
      "source": [
        "### Installation\n",
        "\n",
        "Install ArkIntelligence SDK from Github repository. This may take more than 1 minute in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0T9598P80u26",
        "outputId": "4e6057cd-f8c8-4e67-9d5a-e2fd1ea3305a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/LotsoTeddy/ArkIntelligence.git\n",
            "  Cloning https://github.com/LotsoTeddy/ArkIntelligence.git to /tmp/pip-req-build-73l_wxaf\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/LotsoTeddy/ArkIntelligence.git /tmp/pip-req-build-73l_wxaf\n",
            "  Resolved https://github.com/LotsoTeddy/ArkIntelligence.git to commit 73808beef682853326de2746386eacfbd58d4a5a\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from ArkIntelligence==1.0.0) (2.32.3)\n",
            "Requirement already satisfied: openai>=1.0 in /usr/local/lib/python3.11/dist-packages (from ArkIntelligence==1.0.0) (1.69.0)\n",
            "Requirement already satisfied: docstring_parser in /usr/local/lib/python3.11/dist-packages (from ArkIntelligence==1.0.0) (0.16)\n",
            "Collecting volcengine (from ArkIntelligence==1.0.0)\n",
            "  Downloading volcengine-1.0.179.tar.gz (349 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.3/349.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting llama-index (from ArkIntelligence==1.0.0)\n",
            "  Downloading llama_index-0.12.28-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting loguru (from ArkIntelligence==1.0.0)\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting volcengine-python-sdk[ark] (from ArkIntelligence==1.0.0)\n",
            "  Downloading volcengine-python-sdk-1.1.2.tar.gz (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0->ArkIntelligence==1.0.0) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0->ArkIntelligence==1.0.0) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0->ArkIntelligence==1.0.0) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0->ArkIntelligence==1.0.0) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0->ArkIntelligence==1.0.0) (2.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.0->ArkIntelligence==1.0.0) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0->ArkIntelligence==1.0.0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0->ArkIntelligence==1.0.0) (4.13.0)\n",
            "Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama-index->ArkIntelligence==1.0.0)\n",
            "  Downloading llama_index_agent_openai-0.4.6-py3-none-any.whl.metadata (727 bytes)\n",
            "Collecting llama-index-cli<0.5.0,>=0.4.1 (from llama-index->ArkIntelligence==1.0.0)\n",
            "  Downloading llama_index_cli-0.4.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-core<0.13.0,>=0.12.28 (from llama-index->ArkIntelligence==1.0.0)\n",
            "  Downloading llama_index_core-0.12.28-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama-index->ArkIntelligence==1.0.0)\n",
            "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index->ArkIntelligence==1.0.0)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.10-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama-index->ArkIntelligence==1.0.0)\n",
            "  Downloading llama_index_llms_openai-0.3.29-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 (from llama-index->ArkIntelligence==1.0.0)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl.metadata (726 bytes)\n",
            "Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama-index->ArkIntelligence==1.0.0)\n",
            "  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama-index->ArkIntelligence==1.0.0)\n",
            "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
            "Collecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama-index->ArkIntelligence==1.0.0)\n",
            "  Downloading llama_index_readers_file-0.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index->ArkIntelligence==1.0.0)\n",
            "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index->ArkIntelligence==1.0.0) (3.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->ArkIntelligence==1.0.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->ArkIntelligence==1.0.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->ArkIntelligence==1.0.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->ArkIntelligence==1.0.0) (2025.1.31)\n",
            "Collecting retry==0.9.2 (from volcengine->ArkIntelligence==1.0.0)\n",
            "  Downloading retry-0.9.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting pytz==2020.5 (from volcengine->ArkIntelligence==1.0.0)\n",
            "  Downloading pytz-2020.5-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Collecting pycryptodome==3.9.9 (from volcengine->ArkIntelligence==1.0.0)\n",
            "  Downloading pycryptodome-3.9.9.tar.gz (15.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: protobuf>=3.18.3 in /usr/local/lib/python3.11/dist-packages (from volcengine->ArkIntelligence==1.0.0) (5.29.4)\n",
            "Collecting google>=3.0.0 (from volcengine->ArkIntelligence==1.0.0)\n",
            "  Downloading google-3.0.0-py2.py3-none-any.whl.metadata (627 bytes)\n",
            "Requirement already satisfied: six>=1.0 in /usr/local/lib/python3.11/dist-packages (from volcengine->ArkIntelligence==1.0.0) (1.17.0)\n",
            "Requirement already satisfied: decorator>=3.4.2 in /usr/local/lib/python3.11/dist-packages (from retry==0.9.2->volcengine->ArkIntelligence==1.0.0) (4.4.2)\n",
            "Collecting py<2.0.0,>=1.4.26 (from retry==0.9.2->volcengine->ArkIntelligence==1.0.0)\n",
            "  Downloading py-1.11.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from volcengine-python-sdk[ark]->ArkIntelligence==1.0.0) (2.8.2)\n",
            "Requirement already satisfied: cryptography<43.0.4,>=43.0.3 in /usr/local/lib/python3.11/dist-packages (from volcengine-python-sdk[ark]->ArkIntelligence==1.0.0) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<43.0.4,>=43.0.3->volcengine-python-sdk[ark]->ArkIntelligence==1.0.0) (1.17.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from google>=3.0.0->volcengine->ArkIntelligence==1.0.0) (4.13.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>=1.0->ArkIntelligence==1.0.0) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0->ArkIntelligence==1.0.0) (0.14.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.28->llama-index->ArkIntelligence==1.0.0) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.28->llama-index->ArkIntelligence==1.0.0) (2.0.40)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.28->llama-index->ArkIntelligence==1.0.0) (3.11.14)\n",
            "Collecting banks<3.0.0,>=2.0.0 (from llama-index-core<0.13.0,>=0.12.28->llama-index->ArkIntelligence==1.0.0)\n",
            "  Downloading banks-2.1.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting dataclasses-json (from llama-index-core<0.13.0,>=0.12.28->llama-index->ArkIntelligence==1.0.0)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.28->llama-index->ArkIntelligence==1.0.0) (1.2.18)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.28->llama-index->ArkIntelligence==1.0.0)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.13.0,>=0.12.28->llama-index->ArkIntelligence==1.0.0)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.28->llama-index->ArkIntelligence==1.0.0) (2025.3.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.28->llama-index->ArkIntelligence==1.0.0) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.28->llama-index->ArkIntelligence==1.0.0) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.28->llama-index->ArkIntelligence==1.0.0) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.28->llama-index->ArkIntelligence==1.0.0) (11.1.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.28->llama-index->ArkIntelligence==1.0.0) (9.0.0)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.13.0,>=0.12.28->llama-index->ArkIntelligence==1.0.0)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.13.0,>=0.12.28->llama-index->ArkIntelligence==1.0.0)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.28->llama-index->ArkIntelligence==1.0.0) (1.17.2)\n",
            "Collecting llama-cloud<0.2.0,>=0.1.13 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index->ArkIntelligence==1.0.0)\n",
            "  Downloading llama_cloud-0.1.17-py3-none-any.whl.metadata (902 bytes)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index->ArkIntelligence==1.0.0) (2.2.2)\n",
            "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index->ArkIntelligence==1.0.0)\n",
            "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index->ArkIntelligence==1.0.0)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->ArkIntelligence==1.0.0)\n",
            "  Downloading llama_parse-0.6.4.post1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index->ArkIntelligence==1.0.0) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index->ArkIntelligence==1.0.0) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index->ArkIntelligence==1.0.0) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai>=1.0->ArkIntelligence==1.0.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai>=1.0->ArkIntelligence==1.0.0) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai>=1.0->ArkIntelligence==1.0.0) (0.4.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.28->llama-index->ArkIntelligence==1.0.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.28->llama-index->ArkIntelligence==1.0.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.28->llama-index->ArkIntelligence==1.0.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.28->llama-index->ArkIntelligence==1.0.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.28->llama-index->ArkIntelligence==1.0.0) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.28->llama-index->ArkIntelligence==1.0.0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.28->llama-index->ArkIntelligence==1.0.0) (1.18.3)\n",
            "Collecting griffe (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.28->llama-index->ArkIntelligence==1.0.0)\n",
            "  Downloading griffe-1.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.28->llama-index->ArkIntelligence==1.0.0) (3.1.6)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.28->llama-index->ArkIntelligence==1.0.0) (4.3.7)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->google>=3.0.0->volcengine->ArkIntelligence==1.0.0) (2.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<43.0.4,>=43.0.3->volcengine-python-sdk[ark]->ArkIntelligence==1.0.0) (2.22)\n",
            "Collecting llama-cloud-services>=0.6.4 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->ArkIntelligence==1.0.0)\n",
            "  Downloading llama_cloud_services-0.6.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.28->llama-index->ArkIntelligence==1.0.0) (3.1.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.28->llama-index->ArkIntelligence==1.0.0)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13.0,>=0.12.28->llama-index->ArkIntelligence==1.0.0)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index->ArkIntelligence==1.0.0) (2025.2)\n",
            "Collecting python-dotenv<2.0.0,>=1.0.1 (from llama-cloud-services>=0.6.4->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->ArkIntelligence==1.0.0)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.28->llama-index->ArkIntelligence==1.0.0) (24.2)\n",
            "Collecting colorama>=0.4 (from griffe->banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.28->llama-index->ArkIntelligence==1.0.0)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.28->llama-index->ArkIntelligence==1.0.0) (3.0.2)\n",
            "Downloading llama_index-0.12.28-py3-none-any.whl (7.0 kB)\n",
            "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2020.5-py2.py3-none-any.whl (510 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retry-0.9.2-py2.py3-none-any.whl (8.0 kB)\n",
            "Downloading google-3.0.0-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_agent_openai-0.4.6-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.4.1-py3-none-any.whl (28 kB)\n",
            "Downloading llama_index_core-0.12.28-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.6.10-py3-none-any.whl (14 kB)\n",
            "Downloading llama_index_llms_openai-0.3.29-py3-none-any.whl (23 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.4.7-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading banks-2.1.1-py3-none-any.whl (28 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading llama_cloud-0.1.17-py3-none-any.whl (253 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.9/253.9 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.6.4.post1-py3-none-any.whl (4.9 kB)\n",
            "Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading llama_cloud_services-0.6.9-py3-none-any.whl (29 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading griffe-1.7.2-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.2/129.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Building wheels for collected packages: ArkIntelligence, volcengine, pycryptodome, volcengine-python-sdk\n",
            "  Building wheel for ArkIntelligence (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ArkIntelligence: filename=ArkIntelligence-1.0.0-py3-none-any.whl size=16324 sha256=a7210b761354f64f6b565a80eede0c07bd557d4ef579e19c0b8657b014b8e4e9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-pzj5m8j1/wheels/ed/52/7a/70cc82ba378a2080ddca62c8ee7b018d2cd9a9a9743bf011f8\n",
            "  Building wheel for volcengine (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for volcengine: filename=volcengine-1.0.179-py3-none-any.whl size=710832 sha256=9d3f39e95bd525e5c6e562f4bdc83c872d53aebe9fdc9840cf2bc986eb5e3223\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/da/06/158c8e34a4ff69fc3e89349009ecaabd2d6767a56b6a3f4029\n",
            "  Building wheel for pycryptodome (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycryptodome: filename=pycryptodome-3.9.9-cp311-cp311-linux_x86_64.whl size=13645222 sha256=73bb1fd08d287d87ee57827352bd5a66ae85f26d3decc0a82a27480f7c664052\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/f3/67/ba3e5eafa5abc55a6cd927c2e0c4eabc662a970a9afae3eab4\n",
            "  Building wheel for volcengine-python-sdk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for volcengine-python-sdk: filename=volcengine_python_sdk-1.1.2-py3-none-any.whl size=12008215 sha256=3e7deed962f3374bdbfd7bbf3674c0686c3acb32c6f1ef0ee2dcb59d5ed304e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/26/5a/2a98374f34dc41384b99703de28fb5324f1cd3770d6e4f3f87\n",
            "Successfully built ArkIntelligence volcengine pycryptodome volcengine-python-sdk\n",
            "Installing collected packages: striprtf, pytz, filetype, dirtyjson, python-dotenv, pypdf, pycryptodome, py, mypy-extensions, marshmallow, loguru, colorama, volcengine-python-sdk, typing-inspect, tiktoken, retry, griffe, google, volcengine, llama-cloud, dataclasses-json, banks, llama-index-core, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-readers-llama-parse, llama-index-program-openai, llama-index-question-gen-openai, llama-index, ArkIntelligence\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2025.2\n",
            "    Uninstalling pytz-2025.2:\n",
            "      Successfully uninstalled pytz-2025.2\n",
            "  Attempting uninstall: google\n",
            "    Found existing installation: google 2.0.3\n",
            "    Uninstalling google-2.0.3:\n",
            "      Successfully uninstalled google-2.0.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yfinance 0.2.55 requires pytz>=2022.5, but you have pytz 2020.5 which is incompatible.\n",
            "ibis-framework 9.5.0 requires pytz>=2022.7, but you have pytz 2020.5 which is incompatible.\n",
            "bigframes 1.42.0 requires pytz>=2022.7, but you have pytz 2020.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ArkIntelligence-1.0.0 banks-2.1.1 colorama-0.4.6 dataclasses-json-0.6.7 dirtyjson-1.0.8 filetype-1.2.0 google-3.0.0 griffe-1.7.2 llama-cloud-0.1.17 llama-cloud-services-0.6.9 llama-index-0.12.28 llama-index-agent-openai-0.4.6 llama-index-cli-0.4.1 llama-index-core-0.12.28 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.10 llama-index-llms-openai-0.3.29 llama-index-multi-modal-llms-openai-0.4.3 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.7 llama-index-readers-llama-parse-0.4.0 llama-parse-0.6.4.post1 loguru-0.7.3 marshmallow-3.26.1 mypy-extensions-1.0.0 py-1.11.0 pycryptodome-3.9.9 pypdf-5.4.0 python-dotenv-1.1.0 pytz-2020.5 retry-0.9.2 striprtf-0.0.26 tiktoken-0.9.0 typing-inspect-0.9.0 volcengine-1.0.179 volcengine-python-sdk-1.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/LotsoTeddy/ArkIntelligence.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILGi2_YG0u26"
      },
      "source": [
        "### Authentication\n",
        "\n",
        "Go to [this doc](https://www.volcengine.com/docs/82379/1399008#b00dee71) to learn how to generate your API key, and set it in your code or environment variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8IXRcOG0u26"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"ARK_API_KEY\"] = \"SET_YOUR_ARK_API_KEY\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you run in Google Colab, you can set your api key by the following method in a more private method:"
      ],
      "metadata": {
        "id": "5LzTBjseaWw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from google.colab import userdata\n",
        "os.environ[\"ARK_API_KEY\"] = userdata.get('ARK_API_KEY')"
      ],
      "metadata": {
        "id": "lIESHAidaewY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quEtH1_k0u26"
      },
      "source": [
        "## Quickstart\n",
        "\n",
        "You can chat with a model to learn the model's information:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdDzL7NZ0u26",
        "outputId": "e51d296d-2872-4b0e-a751-508050fc8161"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from model:\n",
            "\n",
            "I'm Doubao, an AI developed by ByteDance. I can answer a wide range of questions, offer information on various topics, have conversations with you, and help you solve different problems. Just tell me what you need! \n"
          ]
        }
      ],
      "source": [
        "from arkintelligence.model import ArkModel\n",
        "\n",
        "model = ArkModel(model=\"doubao-1.5-pro-32k-250115\")\n",
        "\n",
        "response = model.chat(prompt=\"Who are you?\")\n",
        "\n",
        "print('Response from model:\\n')\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzNUWEfS0u27"
      },
      "source": [
        "Or, you can create an agent (named by Translator) for translating your text from *English* to *Chinese*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-na7hpMK0u27",
        "outputId": "7c01c26b-d955-4da9-f509-ad454672d4b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translated text from agent:\n",
            "\n",
            "激发创意，丰富生活！\n"
          ]
        }
      ],
      "source": [
        "from arkintelligence.agent import ArkAgent\n",
        "\n",
        "agent = ArkAgent(\n",
        "    name=\"Translator\",\n",
        "    model=\"doubao-1.5-pro-32k-250115\",\n",
        "    prompt=\"Translate the input text from English to Chinese.\",\n",
        ")\n",
        "\n",
        "response = agent.run(\"Inspire Creativity, Enrich Life!\")\n",
        "\n",
        "print('Translated text from agent:\\n')\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7XyOQM30u27"
      },
      "source": [
        "# Basic usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86v3HcBsIdLf"
      },
      "source": [
        "## Overview\n",
        "\n",
        "The entire list of model ID can be found [here](https://www.volcengine.com/docs/82379/1330310). The capabilities of each model is listed as follows:\n",
        "\n",
        "| Model ID      | Image understanding | Video generation | Function calling |\n",
        "| - | - | - | - |\n",
        "| doubao-1.5-vision-pro-32k-250115 | ✅ | | |\n",
        "| doubao-seaweed-241128 | | ✅ | |\n",
        "| ... | | | |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzm3g2RQGQIh"
      },
      "source": [
        "## Text capabilities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU_eb9k5GW_-"
      },
      "source": [
        "### Chat\n",
        "\n",
        "A simplest chat is in the form of single-turn, which has no memory. The history messages whether from user or model will not be saved. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgT1DSPH0u27",
        "outputId": "f183cb94-8620-4a1c-890d-b4227fbc75ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response from the first chat:\n",
            "\n",
            "Got it! From now on, my name is ArkIntelligence. Nice to meet you!\n",
            "\n",
            "\n",
            "Response from the second chat:\n",
            "\n",
            "I don't remember the last prompt as I don't have a memory of previous interactions in that way. My name is Doubao. \n"
          ]
        }
      ],
      "source": [
        "from arkintelligence.model import ArkModel\n",
        "\n",
        "model = ArkModel(model=\"doubao-1.5-pro-32k-250115\")\n",
        "\n",
        "res1 = model.chat(prompt=\"Your name is ArkIntelligence.\")\n",
        "res2 = model.chat(prompt=\"Do you remember the last prompt? What is your name?\")\n",
        "\n",
        "print('Response from the first chat:\\n')\n",
        "print(res1)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print('Response from the second chat:\\n')\n",
        "print(res2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_S5v1-36CT3"
      },
      "source": [
        "In the above code, the first chat sets a name for the model, but this message is not saved, hence the second chat will not return the preset name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLfun-a8S5tP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gpFnnfV0u27"
      },
      "source": [
        "### Chat with memory\n",
        "\n",
        "Sometimes you need a multiple turn chatting, you can enable history message saving by setting `enable_context=True` during model initialization. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dy8IJ2vI0u27",
        "outputId": "32b3c508-6bc6-495f-e188-f218dd9f8d4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response from the first chat:\n",
            "\n",
            "Thank you for naming me ArkIntelligence. From now on, I'll serve you under this name! If you have any questions, just tell me. \n",
            "\n",
            "\n",
            "Response from the second chat:\n",
            "\n",
            "My name is ArkIntelligence. I remember you assigned this name to me in the previous prompt.\n"
          ]
        }
      ],
      "source": [
        "from arkintelligence.model import ArkModel\n",
        "\n",
        "model = ArkModel(\n",
        "    model=\"doubao-1.5-pro-32k-250115\",\n",
        "    enbale_context=True # Make the model remember the context\n",
        "    )\n",
        "\n",
        "res1 = model.chat(prompt=\"Your name is ArkIntelligence.\")\n",
        "res2 = model.chat(prompt=\"Do you remember the last prompt? What is your name?\")\n",
        "\n",
        "print('Response from the first chat:\\n')\n",
        "print(res1)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print('Response from the second chat:\\n')\n",
        "print(res2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5ntuS7B0u27"
      },
      "source": [
        "The model can remember the previous user inputs. The context will be managed automatically in ArkIntelligence!\n",
        "\n",
        "### Chat with attachment [WIP]\n",
        "\n",
        "We support upload your single file with format of `.txt`, for example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwvyrYsV0u27"
      },
      "outputs": [],
      "source": [
        "# ======== [WIP] ========\n",
        "# from arkintelligence.model import ArkModel\n",
        "\n",
        "# model = ArkModel(model=\"doubao-1.5-pro-32k-250115\")\n",
        "\n",
        "# response = model.chat(\n",
        "#     prompt=\"Your name is ArkIntelligence.\",\n",
        "#     attachment=\"FILE_PATH\",  # TODO(LotsoTeddy): Parsing attachment\n",
        "# )\n",
        "# response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGIgSZBa0u27"
      },
      "source": [
        "## Vision capabilities\n",
        "\n",
        "Ark provides capabilities about multi-media, such as vision and sounds. Here we introduce the vision-related demos. The vision-related task is devided into image understanding and video generation.\n",
        "\n",
        "- **Image understanding**: this task can read information from one or several images and return the content to the user\n",
        "- **Video generation**: this task can generate video from text and images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULwq7QJhGeae"
      },
      "source": [
        "### Image understanding\n",
        "\n",
        "We use the model `doubao-1.5-vision-pro-32k-250115` to understand the following image:\n",
        "\n",
        "<img src='https://ark-tutorial.tos-cn-beijing.volces.com/assets/images/cat.png' style='width:100px'>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vk9lq6bB0u27",
        "outputId": "50873e74-4e2b-4feb-a041-5ef747dfb2f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response from model:\n",
            "\n",
            "This is a close - up photograph of a charming cat. The cat has a soft, light gray coat with subtle darker gray stripes running through it, giving its fur a delicate and textured appearance. Its face is round and endearing, with large, round eyes that are a dark, captivating shade, making the cat look incredibly alert and curious. The cat's nose is small and pink, adding a touch of cuteness to its overall look.\n",
            "\n",
            "Long, white whiskers extend from either side of its muzzle, emphasizing its feline features. The cat's ears are upright and have a light pink inner lining, covered with fine fur. It is lying down on what appears to be a light - colored surface, possibly a carpet or a mat.\n",
            "\n",
            "In the background, there are some indistinct objects. There is a glimpse of what seems to be a piece of furniture, perhaps a chair with a dark backrest, and some other household items that are out of focus, ensuring that the cat remains the central subject of the image. The overall atmosphere of the picture is cozy and intimate, capturing a moment of the cat's relaxed state at home. \n"
          ]
        }
      ],
      "source": [
        "from arkintelligence.model import ArkModel\n",
        "\n",
        "IMAGE_PATH = \"https://ark-tutorial.tos-cn-beijing.volces.com/assets/images/cat.png\"\n",
        "model = ArkModel(\n",
        "    model=\"doubao-1.5-vision-pro-32k-250115\",  # Use vision model here\n",
        ")\n",
        "\n",
        "response = model.process_image(\n",
        "    prompt=\"Please describe this image with details.\",\n",
        "    attachment=IMAGE_PATH,\n",
        ")\n",
        "\n",
        "print('Response from model:\\n')\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kycGTtYd0u27"
      },
      "source": [
        "### Video generation\n",
        "\n",
        "We use `doubao-seaweed-241128` model to generate a video according to a static image and prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eC7f0ipR0u27",
        "outputId": "d5a0f822-9d66-4d9b-a6f6-eb0e7859564e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Waiting for video generation, this may take a while...\n",
            "\n",
            "Generated video url: https://ark-content-generation-cn-beijing.tos-cn-beijing.volces.com/doubao-seaweed/doubao-seaweed-2100390175-02174357478596100000000000000000000ffffac15606b93ad78.mp4?X-Tos-Algorithm=TOS4-HMAC-SHA256&X-Tos-Credential=AKLTYjg3ZjNlOGM0YzQyNGE1MmI2MDFiOTM3Y2IwMTY3OTE%2F20250402%2Fcn-beijing%2Ftos%2Frequest&X-Tos-Date=20250402T062038Z&X-Tos-Expires=86400&X-Tos-Signature=d3e06675512df758b34e4e64c10a1ebdcbd32bddaf7e6eb67c29177b46cb7b27&X-Tos-SignedHeaders=host\n"
          ]
        }
      ],
      "source": [
        "REF_IMAGE_PATH = \"https://ark-tutorial.tos-cn-beijing.volces.com/assets/images/cat.png\"\n",
        "model = ArkModel(\n",
        "    model=\"doubao-seaweed-241128\",  # Use video generation model here\n",
        ")\n",
        "\n",
        "response = model.generate_video(\n",
        "    prompt=\"Please generate a video with a cat running.\",\n",
        "    attachment=REF_IMAGE_PATH,\n",
        ")\n",
        "\n",
        "# This may take a while...\n",
        "print(f\"Generated video url: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYcHPock0u27"
      },
      "source": [
        "For more models that support video generation, you can visit [here]().\n",
        "\n",
        "If you want to make the video more vivid, maybe you need: prompt refine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqWSml5i0u28"
      },
      "source": [
        "# Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta50kBeYKBFM"
      },
      "source": [
        "## A minimal agent\n",
        "\n",
        "A simple agent can be built with several lines. The `name` field is not necessary, but provide it will make agent more intelligent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejpD_lRz0u28"
      },
      "outputs": [],
      "source": [
        "from arkintelligence.agent import ArkAgent\n",
        "\n",
        "agent = ArkAgent(\n",
        "    name=\"Meeting assistant\",\n",
        "    model=\"deepseek-v3-250324\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqEpIpze0u28"
      },
      "source": [
        "Then, you can run it with an input prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "Cqz2hfXy0u28",
        "outputId": "ead5a37e-e022-46d5-b97f-1e9fb75b8d32"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hi there! 👋 I\\'m your **Meeting Assistant**, here to make your meetings smoother and more productive. Here\\'s what I can do for you:\\n\\n### **Who I Am**  \\nI’m an AI-powered assistant designed to help with meeting-related tasks—whether it’s scheduling, note-taking, summarizing discussions, or follow-ups.\\n\\n### **What I Can Do**  \\n✅ **Schedule & Coordinate Meetings** – Find the best time for everyone, send invites, and handle reminders.  \\n✅ **Take Notes** – Capture key points, decisions, and action items in real time (if integrated with your meeting tools).  \\n✅ **Summarize Discussions** – Provide concise recaps with highlights, deadlines, and next steps.  \\n✅ **Generate Follow-ups** – Draft emails or task lists based on meeting outcomes.  \\n✅ **Answer Questions** – Need info from past meetings? I can help retrieve details.  \\n\\nLet me know how I can assist—just give me the details! 🚀  \\n\\n*(Example: \"Set up a 30-minute team meeting next week\" or \"Summarize this transcript.\")*'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = agent.run(\"Who are you and what can you do?\")\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8OCLym00u28"
      },
      "source": [
        "Introduce what the agent is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I2sFFUVKjK7"
      },
      "source": [
        "## Prompt engineering\n",
        "\n",
        "Prompt engineering is important that can make your prompt more rich and useful for models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTTDeIAxKjNO"
      },
      "source": [
        "### Prompt usage\n",
        "\n",
        "Prompt can be used for interacting with models. The models understand your prompt and give responses.\n",
        "\n",
        "For example, with a prompt, a complex English statement can be optimized to be more concise:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1qrs9s10u28",
        "outputId": "27588e71-7692-43cc-e20d-6869626c3ce0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Q: I will give you a sentence, please make the sentence more concise and elegant.\n",
            "\n",
            "A: Sure! Please provide the sentence, and I'll do my best to make it more concise and elegant.\n",
            "\n",
            "\n",
            "\n",
            "Q: In a Chinese house, the kitchen is only a place for cooking things; but in many Western houses, the kitchen is not only a place where people cook meals and eat them but also a place where the family members or friends usually meet each other.\n",
            "\n",
            "A: In Chinese houses, the kitchen serves solely for cooking. In contrast, in many Western homes, it's not just a cooking and dining area but also a gathering place for family and friends. \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from arkintelligence.model import ArkModel\n",
        "\n",
        "model = ArkModel(\n",
        "    model=\"doubao-1.5-pro-32k-250115\",\n",
        "    enbale_context=True,\n",
        ")\n",
        "\n",
        "prompt1 = \"I will give you a sentence, please make the sentence more concise and elegant.\"\n",
        "prompt2 = \"In a Chinese house, the kitchen is only a place for cooking things; but in many Western houses, the kitchen is not only a place where people cook meals and eat them but also a place where the family members or friends usually meet each other.\"\n",
        "\n",
        "res1 = model.chat(prompt1)\n",
        "res2 = model.chat(prompt2)\n",
        "\n",
        "print(f'''\n",
        "Q: {prompt1}\\n\n",
        "A: {res1}\\n\n",
        "\\n\n",
        "Q: {prompt2}\\n\n",
        "A: {res2}\\n\n",
        "''')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2nZWUb70u28"
      },
      "source": [
        "### Prompt refine\n",
        "\n",
        "Refine prompts is important, the comparision is as follows. We use a simple and a refined prompt to generate images, then compare the image quality.\n",
        "\n",
        "In fact, you can build an agent to refine prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_21pcLI0u28",
        "outputId": "aa34f896-8fea-4400-c891-f9376e9bed6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original prompt: Draw a cute golden british shorthair cat.\n",
            "\n",
            "\n",
            "Refined prompt: Create an image of an adorably cute Golden British Shorthair cat. Focus on capturing the breed's characteristic round face, plush coat with a golden hue, and large, expressive eyes. Ensure the cat has a friendly, endearing pose, perhaps sitting upright, with its soft fur looking fluffy and inviting. Use warm, soft - toned lighting to enhance the cuteness and give the overall image a cozy, appealing aesthetic.\n"
          ]
        }
      ],
      "source": [
        "from arkintelligence.agent import ArkAgent\n",
        "\n",
        "prompt = \"Draw a cute golden british shorthair cat.\"\n",
        "\n",
        "refine_agent = ArkAgent(\n",
        "    name=\"Prompt refine assistant\",\n",
        "    model=\"doubao-1-5-pro-256k-250115\",\n",
        "    prompt=\"Refine the prompt to make it more suitable for image generation.\",\n",
        ")\n",
        "prompt_refined = refine_agent.run(prompt)\n",
        "\n",
        "print(f\"Original prompt: {prompt}\")\n",
        "print('\\n')\n",
        "print(f\"Refined prompt: {prompt_refined}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTogTxvV0u28"
      },
      "source": [
        "Then we use the two prompts to generate videos and see the differents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KORB226K0u28",
        "outputId": "66df5b99-f592-46ce-f2c8-355fb48d9bf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original video url is: https://ark-content-generation-cn-beijing.tos-cn-beijing.volces.com/doubao-seaweed/doubao-seaweed-2100390175-02174357522993500000000000000000000ffffac15606b87092c.mp4?X-Tos-Algorithm=TOS4-HMAC-SHA256&X-Tos-Credential=AKLTYjg3ZjNlOGM0YzQyNGE1MmI2MDFiOTM3Y2IwMTY3OTE%2F20250402%2Fcn-beijing%2Ftos%2Frequest&X-Tos-Date=20250402T062757Z&X-Tos-Expires=86400&X-Tos-Signature=ed38c5457d2a92b48c4ba79df28c4607efa94aeec1f82fb876a666723095a4e2&X-Tos-SignedHeaders=host\n",
            "Refined video url is: https://ark-content-generation-cn-beijing.tos-cn-beijing.volces.com/doubao-seaweed/doubao-seaweed-2100390175-02174357528376500000000000000000000ffffac15606b5a5252.mp4?X-Tos-Algorithm=TOS4-HMAC-SHA256&X-Tos-Credential=AKLTYjg3ZjNlOGM0YzQyNGE1MmI2MDFiOTM3Y2IwMTY3OTE%2F20250402%2Fcn-beijing%2Ftos%2Frequest&X-Tos-Date=20250402T062851Z&X-Tos-Expires=86400&X-Tos-Signature=92940cc3b3ae17a1b52864371249f699f716d95f3f28ede79255aa9f1853235c&X-Tos-SignedHeaders=host\n"
          ]
        }
      ],
      "source": [
        "from arkintelligence.model import ArkModel\n",
        "\n",
        "model = ArkModel(\n",
        "    model=\"doubao-seaweed-241128\",  # Use video generation model here\n",
        ")\n",
        "\n",
        "video = model.generate_video(\n",
        "    prompt=prompt,\n",
        ")\n",
        "video_with_refine = model.generate_video(\n",
        "    prompt=prompt_refined,\n",
        ")\n",
        "\n",
        "print(f'Original video url is: {video}')\n",
        "print(f'Refined video url is: {video_with_refine}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dB6KcEvV0u28"
      },
      "source": [
        "### Equip to agent [WIP]\n",
        "\n",
        "You can enable prompt refine in your agent, the agent will automatically refine your **first** prompt with a default refine prompt (you can modify this by pass `refine_prompt`). The usage is as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IZDBjdl0u28"
      },
      "outputs": [],
      "source": [
        "# TODO, WIP\n",
        "\n",
        "# from arkintelligence.agent import ArkAgent\n",
        "\n",
        "# prompt = \"Draw a cute golden british shorthair cat.\"\n",
        "\n",
        "# refine_agent = ArkAgent(\n",
        "#     name=\"Prompt refine assistant\",\n",
        "#     model=\"doubao-1-5-pro-256k-250115\",\n",
        "#     prompt=\"Refine the prompt to make it more suitable for image generation.\",\n",
        "#     refine_requirement=\"Refine the prompt to make it more suitable for image generation.\",\n",
        "# )\n",
        "# prompt_refined = refine_agent.run(prompt)\n",
        "\n",
        "# print(f\"Original prompt:\\n{prompt}\")\n",
        "# print(f\"Refined prompt:\\n{prompt_refined}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P1okD4n0u28"
      },
      "source": [
        "## Function calling\n",
        "\n",
        "The Ark agent can call your local function to finish your task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dD0NiTO8L303"
      },
      "source": [
        "### Create a tool\n",
        "\n",
        "Before initializing an agent, you need to create a tool (which is just a Python function) to define tool logic. For example, we provide a `visit_url` here to read the website information:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UBRiXznW0u28"
      },
      "outputs": [],
      "source": [
        "from arkintelligence.tool import ArkTool\n",
        "\n",
        "\n",
        "@ArkTool\n",
        "def visit_url(url: str):\n",
        "    \"\"\"Visit a URL and return the content.\n",
        "\n",
        "    This function can receive an url, and request to the url, then get the content of this url.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL to visit, generally begins with `http`.\n",
        "\n",
        "    Returns:\n",
        "        str: The content of the URL.\n",
        "    \"\"\"\n",
        "    response = \"This url introduces ArkIntelligence, including basic usage, agent building, and other advanced development methdology.\"\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7m7weHn0u28"
      },
      "source": [
        "Any function can be decorated by `ArkTool` to be a tool, which can be invoked by Ark agent.\n",
        "\n",
        "\n",
        "**NOTE:** The docstring of function is important, as its name, description and arguments will be sent to the model. The detailed docstring usage can be found [here]()."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hr7ZLxqMLqd"
      },
      "source": [
        "### Equip to agent\n",
        "\n",
        "You need to use the model which can support function calling.\n",
        "\n",
        "\n",
        "The created tool can be equipped to an agent with just only one option:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxAHNWEl0u28",
        "outputId": "b23f75df-5573-4125-ff4f-a397f1a28257"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The content of 'https://www.foo.bar.com/' introduces ArkIntelligence, including basic usage, agent building, and other advanced development methodology. \n"
          ]
        }
      ],
      "source": [
        "from arkintelligence.agent import ArkAgent\n",
        "\n",
        "agent = ArkAgent(\n",
        "    name=\"Web search assistant\",\n",
        "    model=\"doubao-1.5-pro-32k-250115\",\n",
        "    tools=[\"visit_url\"],\n",
        ")\n",
        "\n",
        "response = agent.run(\"What is the content of 'https://www.foo.bar.com/'?\")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoQF4T_tNt9z"
      },
      "source": [
        "In the output, the model can return our preset website content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-ZC3WfA0u28"
      },
      "source": [
        "## RAG\n",
        "\n",
        "RAG enhances model response. In Ark, we provide concise method to enbale RAG.\n",
        "\n",
        "### Knowledge base\n",
        "\n",
        "You can create a knowledge base with your local files like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATH0cDxq0u28"
      },
      "outputs": [],
      "source": [
        "from arkintelligence.knowledgebase import ArkKnowledgeBase\n",
        "\n",
        "kb = ArkKnowledgeBase(\n",
        "    name=\"ArkIntelligence\",\n",
        "    description=\"ArkIntelligence is a company that provides AI solutions.\",\n",
        "    data=data\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rXAIUjh0u28"
      },
      "source": [
        "During creation, your data will be uploaded to Ark platform and processed by embedding models such as `doubao-embed` (embed model API can be found [here]()). The processed data is stored in your local memory rather than cloud space.\n",
        "\n",
        "### Equip to agent\n",
        "\n",
        "Equip the knowledge base to your agent like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Afk8aV2O0u28"
      },
      "outputs": [],
      "source": [
        "agent = ArkAgent(\n",
        "    name=\"Knowledge base agent\",\n",
        "    model=\"deepseek-v3-250324\",\n",
        "    prompt=\"You are a helpful assistant.\",\n",
        "    knowldgebase=kb,\n",
        ")\n",
        "response = agent.run(\"Summary the pros and cons of SmartVM\")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjGbBt6h0u29"
      },
      "source": [
        "# Awesome samples\n",
        "\n",
        "## Auto-summary\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "arkintelligence",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}